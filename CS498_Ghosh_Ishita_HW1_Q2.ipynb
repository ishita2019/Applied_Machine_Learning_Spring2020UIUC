{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset available at https://archive.ics.uci.edu/ml/datasets/Adult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1= pd.read_csv('./adult.data')\n",
    "data1.columns=['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship',\n",
    "               'race','sex','capital-gain','capital-loss','hours-per-week','native-country','>50K, <=50K.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= pd.read_csv('./adult.test', sep=\",\", header=None)\n",
    "data2.columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship',\n",
    "                 'race','sex','capital-gain','capital-loss','hours-per-week','native-country','>50K, <=50K.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = data1.append(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataxy = final_data.iloc[:,[0,2,4,10,11,12,14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataxy = dataxy.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataxy['>50K, <=50K.'] = np.where(dataxy['>50K, <=50K.'] == '>50K.', '>50K', dataxy['>50K, <=50K.'])\n",
    "dataxy['>50K, <=50K.'] = np.where(dataxy['>50K, <=50K.'] == '>50K', 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale Data using mean and S.D of train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=dataxy.iloc[:,[0,1,2,3,4,5]]\n",
    "labels = dataxy.iloc[:,6]\n",
    "features_normalized = MinMaxScaler().fit_transform(features.values)\n",
    "features = pd.DataFrame(features_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_features, test_features,model_train_labels, test_labels = train_test_split(features, \n",
    "                                                                labels, test_size=0.1, random_state=42)\n",
    "\n",
    "model_train_features, validation_features,model_train_labels, validation_labels = train_test_split(model_train_features, \n",
    "                                                                model_train_labels, test_size=0.111111111111111, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "steps=300\n",
    "#const1=0.1\n",
    "#const2=1\n",
    "held_out = 50\n",
    "acc_steps = 30\n",
    "m = 1\n",
    "n = 10\n",
    "batch=len(model_train_features)//300\n",
    "lambda_vals= [0.0001, 0.001, 0.01, 0.1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, y):\n",
    "    correct = 0\n",
    "    for prediction, actual in zip(predictions, y):\n",
    "        if prediction == actual:\n",
    "            correct += 1\n",
    "    return correct / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially I have worked on the program logic on my own and after developing the logic I have searched for\n",
    "better structures for the same and found the following link very inspirational for reffering: https://github.com/hxiuyu2/AML/blob/master/HW2/SVM.py\n",
    "#I acklowledge the help in polishing the concept here using the rescources from google
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(a, b, nda, lambda_val, input_data, yinput):\n",
    "    res = yinput * (np.dot(a, input_data.T) + b)\n",
    "    if res >= 1:\n",
    "        temp = (nda * lambda_val) * a\n",
    "        a -= temp\n",
    "    if res < 1:\n",
    "        temp = lambda_val * a\n",
    "        temp -= yinput * input_data\n",
    "        a -= nda * temp\n",
    "        b -= nda * (-yinput)\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially I have worked on the program logic on my own and after developing the logic I have searched \n",
    "for better structures for the same and found the following link very inspirational for reffering: https://github.com/hxiuyu2/AML/blob/master/HW2/SVM.py\n",
    "I acklowledge the help in polishing the concept here using the following link: https://github.com/hxiuyu2/AML/blob/master/HW2/SVM.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do mini batch gradient\n",
    "def mini_batch_gradient_cal(batch_size, new_train_data, new_yinput, a, b, nda, lambda_val):\n",
    "    batch_a = np.zeros(6)\n",
    "    batch_b = 0\n",
    "    rand_index = np.random.choice(range(len(new_train_data)), batch)\n",
    "    for batch_ind in rand_index:\n",
    "        # gradient update parameter a and b\n",
    "        xi = new_train_data[batch_ind]\n",
    "        yi = new_yinput[batch_ind]\n",
    "        new_a, new_b = gradient(a, b, nda, lambda_val, xi, yi)\n",
    "        batch_a += new_a\n",
    "        batch_b += new_b\n",
    "\n",
    "    a = (1 / batch) * batch_a\n",
    "    b = (1 / batch) * batch_b\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict class\n",
    "def sign(test_set, a, b):\n",
    "    res = np.dot(test_set, a.T) + b\n",
    "    for i in range(0,len(res)):\n",
    "        if(res[i]>0):\n",
    "            res[i]=1\n",
    "        else:\n",
    "            res[i]=-1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially I have worked on the program logic on my own and after developing the logic I have searched \n",
    "for better structures for the same and found the following link very inspirational for reffering: https://github.com/hxiuyu2/AML/blob/master/HW2/SVM.py\n",
    "I acklowledge the help in polishing the concept here using the following link: https://github.com/hxiuyu2/AML/blob/master/HW2/SVM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM training\n",
    "acc_30_steps = []\n",
    "magnitude = []\n",
    "\n",
    "def svm(lambda_val, train, labels):\n",
    "   \n",
    "    a = np.random.rand(6)\n",
    "    b = np.random.rand(1)[0]\n",
    "    count = 0\n",
    "    stable_acc = 0\n",
    "    for i in range(epochs):\n",
    "        idnx=0\n",
    "        new_model_train_features, new_validation_features, new_model_train_labels, new_validation_labels = \n",
    "        train_test_split(model_train_features.to_numpy(), model_train_labels.to_numpy(), test_size=0.002, random_state=42)\n",
    "        \n",
    "        for j in range(steps):\n",
    "            # update nda for each steps\n",
    "            nda = batch*m / ((i+1)*j + batch*n)\n",
    "            minibatch_train=new_model_train_features[idnx:idnx+batch,:]\n",
    "            minibatch_lables=new_model_train_labels[idnx:idnx+batch]\n",
    "            if(len(minibatch_train)==0):\n",
    "                break\n",
    "            a, b = mini_batch_gradient_cal(batch, minibatch_train, minibatch_lables, a, b, nda, lambda_val)\n",
    "            idnx+=batch\n",
    "            predictions = sign(new_model_train_features, a, b)\n",
    "            season_acc = accuracy(predictions, new_validation_labels)\n",
    "            if stable_acc != season_acc:\n",
    "                stable_acc = season_acc\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "\n",
    "            # calculate accuracy every 30 steps\n",
    "            if (j % acc_steps == 0) & (j > 0):\n",
    "                acc_30_steps.append(season_acc)\n",
    "                magnitude.append(np.sum(a ** 2))\n",
    "        \n",
    "        predictions = sign(new_model_train_features, a, b)\n",
    "        epoch_acc = accuracy(predictions, new_model_train_labels)\n",
    "        acc_30_steps.append(epoch_acc)\n",
    "        print(\"Epochs accuracy is \", epoch_acc)\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially I have worked on the program logic on my own and after developing the logic I have searched \n",
    "for better structures for the same and found the following link very inspirational for reffering: https://github.com/hxiuyu2/AML/blob/master/HW2/SVM.py\n",
    "I acklowledge the help in polishing the concept here using the following link: https://github.com/hxiuyu2/AML/blob/master/HW2/SVM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs accuracy is  0.7812940784243326\n",
      "Epochs accuracy is  0.7871156361398199\n",
      "Epochs accuracy is  0.7977329264226912\n",
      "Epochs accuracy is  0.8024004308465622\n",
      "Epochs accuracy is  0.8018362270151053\n",
      "Epochs accuracy is  0.8011181494114328\n",
      "Epochs accuracy is  0.8019388095299156\n",
      "Epochs accuracy is  0.8036314210242864\n",
      "Epochs accuracy is  0.8004000718077604\n",
      "Epochs accuracy is  0.8054009694047649\n",
      "for lambda =  0.0001  the accuracy on validation set is  0.8085585585585585\n",
      "Epochs accuracy is  0.7652912061139179\n",
      "Epochs accuracy is  0.7727027928089657\n",
      "Epochs accuracy is  0.7681378708999052\n",
      "Epochs accuracy is  0.7703690405970303\n",
      "Epochs accuracy is  0.7749596081347935\n",
      "Epochs accuracy is  0.7794988844151515\n",
      "Epochs accuracy is  0.7726771471802631\n",
      "Epochs accuracy is  0.778652578667966\n",
      "Epochs accuracy is  0.779396301900341\n",
      "Epochs accuracy is  0.7752930013079271\n",
      "for lambda =  0.001  the accuracy on validation set is  0.7798935298935299\n",
      "Epochs accuracy is  0.7597261046854563\n",
      "Epochs accuracy is  0.7632652014464134\n",
      "Epochs accuracy is  0.7628805170158747\n",
      "Epochs accuracy is  0.7640602159361937\n",
      "Epochs accuracy is  0.7624958325853358\n",
      "Epochs accuracy is  0.7629061626445772\n",
      "Epochs accuracy is  0.7618546918677711\n",
      "Epochs accuracy is  0.762854871387172\n",
      "Epochs accuracy is  0.7630856820454953\n",
      "Epochs accuracy is  0.7635216577334394\n",
      "for lambda =  0.01  the accuracy on validation set is  0.7688370188370188\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "for lambda =  0.1  the accuracy on validation set is  0.765970515970516\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "Epochs accuracy is  0.7595978765419434\n",
      "for lambda =  1  the accuracy on validation set is  0.765970515970516\n",
      "best_lambda : 0.0001\n",
      "for best lambda =  1  the accuracy on test set is  0.765970515970516\n"
     ]
    }
   ],
   "source": [
    "# train svm\n",
    "best_l = 0\n",
    "best_acc = 0\n",
    "best_a = 0\n",
    "best_b = 0\n",
    "acc_30_for_All_Lambda=[]\n",
    "for l in lambda_vals:\n",
    "    acc_30_steps = []\n",
    "    a, b = svm(l, model_train_features, model_train_labels)\n",
    "    predictions = sign(validation_features, a, b)\n",
    "    cur_acc = accuracy(predictions, validation_labels)\n",
    "    if cur_acc > best_acc:\n",
    "        best_acc = cur_acc\n",
    "        best_l = l\n",
    "        best_a = a\n",
    "        best_b = b\n",
    "    print(\"for lambda = \", l, \" the accuracy on validation set is \", cur_acc)\n",
    "    plot_acc[\"reg=\"+str(l)] = acc_30_steps\n",
    "    plot_mag[\"reg=\"+str(l)] = magnitude\n",
    "    acc_30_for_All_Lambda.append(acc_30_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_lambda : 0.0001\n",
      "for best lambda =  0.0001  the accuracy on test set is  0.7641760491299897\n"
     ]
    }
   ],
   "source": [
    "print(\"best_lambda :\", best_l )\n",
    "plot_mag['x'] = range(len(acc_30_steps))\n",
    "\n",
    "predictions_best_lambda = sign(test_features, best_a, best_b)\n",
    "best_lambda_acc = accuracy(predictions, test_labels)\n",
    "print(\"for best lambda = \", best_l, \" the accuracy on test set is \", best_lambda_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axes = plt.gca()\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# for l in lambda_vals:\n",
    "#     plt.plot( i in range(0,len(acc_30_steps)),acc_30_steps)\n",
    "# plt.legend()\n",
    "# plt.savefig('Accuracy_per_30_steps.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
